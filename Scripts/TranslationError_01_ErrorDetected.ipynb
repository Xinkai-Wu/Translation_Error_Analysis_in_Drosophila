{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/lustre/user/lulab/doushq/wuxk/software/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import warnings\n",
    "import pickle\n",
    "from Bio import Seq\n",
    "from Bio import SeqIO\n",
    "import pysam\n",
    "import itertools\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import chi2_contingency\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PARAMETERS FOR DETECT\n",
    "\"\"\"\n",
    "\n",
    "#specify a path to the fasta file (DNA sequence of protein coding sequences)\n",
    "path_to_fasta = '../Data/cds_pools_1009.fasta'\n",
    "\n",
    "#path to MaxQuant's table, allPeptides.txt\n",
    "path_to_allPeptides = '../Data/allPeptides.txt'\n",
    "\n",
    "#m/z tolerance, used to filter DP–BP couples that resemble substitutions\n",
    "#and exclude couples that resemble known PTM\n",
    "tol = 0.005\n",
    "\n",
    "#these parameters control the filtering of n-term and c-term modifications\n",
    "n_term_prob_cutoff = 0.05\n",
    "c_term_prob_cutoff = 0.05\n",
    "\n",
    "#defines the minimal threshold on MaxQuant's localisation probability. Only\n",
    "#DP–BP peptides for which the localisation of the modification has be been\n",
    "#determined with higher confidence will be retained. \n",
    "positional_probability_cutoff = 0.95\n",
    "\n",
    "#FDR for the final target-decoy FDR procedure\n",
    "fdr = 0.01\n",
    "\n",
    "# danger_mods were download from https://github.com/ernestmordret/substitutions/blob/master/danger_mods, \n",
    "# could also be built by yourself using the script (https://github.com/ernestmordret/substitutions/blob/master/params.py)\n",
    "path_to_danger_mods = '../Data/danger_mods'\n",
    "\n",
    "\"\"\"\n",
    "PARAMETERS FOR QUANTIFY\n",
    "\"\"\"\n",
    "\n",
    "excluded_samples = []\n",
    "\n",
    "#path to MaxQuant's table, evidence.txt\n",
    "path_to_evidence = '../Data/evidence.txt'\n",
    "\n",
    "#path to MaxQuant's table, matchedFeatures.txt\n",
    "path_to_matched_features = '../Data/matchedFeatures.txt'\n",
    "\n",
    "#path to MaxQuant's table, peptides.txt\n",
    "path_to_peptides = '../Data/peptides.txt'\n",
    "\n",
    "#m/z tolerance for the fetching unidentified features\n",
    "mz_tol = 10 * 10**-6 # 10 ppm\n",
    "\n",
    "#retention time tolerance for the fetching unidentified features\n",
    "rt_tol = 0.3 # min\n",
    "\n",
    "#path to MaxQuant's table, proteinGroups.txt\n",
    "path_to_proteinGroups = '../Data/proteinGroups.txt'\n",
    "\n",
    "# path to MaxQuant's summary, summary.txt\n",
    "path_to_summary = '../Data/summary.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitution detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def codonify(seq):\n",
    "    \"\"\"\n",
    "    input: a nucleotide sequence (not necessarily a string)\n",
    "    output: a list of codons\n",
    "    \"\"\"\n",
    "    seq = str(seq)\n",
    "    l = len(seq)\n",
    "    return [seq[i:i+3] for i in range(0,l,3)]\n",
    "\n",
    "def find_proteins(base_seq):\n",
    "    \"\"\"\n",
    "    input: a peptide sequence (string)\n",
    "    output: the names of proteins containing that sequence \n",
    "    \"\"\"\n",
    "    tbr = \" \".join([names_list[i] for i in np.searchsorted(boundaries_aa-1, SA_search(base_seq, W_aa, sa))])\n",
    "    if tbr.strip(\" \") == '':\n",
    "        return ''\n",
    "    else:\n",
    "        return tbr\n",
    "    \n",
    "def fetch_codon(base_seq, modified_pos):\n",
    "    \"\"\"\n",
    "    input: the original aa sequence of a peptide (base_seq),\n",
    "            and the relative position of the modification.\n",
    "    output: returns the list of all codons possibly associated \n",
    "            with the substitution, presented as a string separated\n",
    "            by white spaces.\n",
    "    \"\"\"\n",
    "    possible_codons = []\n",
    "    proteins = find_proteins(base_seq)\n",
    "    if proteins:\n",
    "        proteins = proteins.split(\" \")\n",
    "    else:\n",
    "        return '_'\n",
    "    for p in proteins:\n",
    "        if p in record_dict:\n",
    "            s = record_dict[p].seq\n",
    "            seq_i = s.translate().find(base_seq)\n",
    "            i = seq_i + modified_pos\n",
    "            possible_codons.append(codonify(s)[i])\n",
    "        else:\n",
    "            possible_codons.append('_')\n",
    "    return \" \".join(possible_codons)\n",
    "\n",
    "def fetch_best_codons(modified_seq):\n",
    "    \"\"\"\n",
    "    input: a modified sequence, e.g. LQV(0.91)A(0.09)EK\n",
    "    output: the list of codons associated with the most likely\n",
    "            position\n",
    "    \"\"\"\n",
    "    possible_sites = re.findall('\\(([^\\)]+)\\)', modified_seq)\n",
    "    best_site = np.argmax([float(i) for i in possible_sites]) # 返回数组中最大概率的氨基酸的索引\n",
    "    modified_pos_prime = [m.start()-1 for m in re.finditer('\\(',modified_seq) ][best_site]\n",
    "    modified_pos = len(re.sub('\\(([^\\)]+)\\)', '',modified_seq[:modified_pos_prime]))\n",
    "    base_seq = re.sub('\\(([^\\)]+)\\)', '', modified_seq)\n",
    "    return fetch_codon(base_seq, modified_pos)\n",
    "\n",
    "def find_substitution_position_local(modified_seq, protein):\n",
    "    \"\"\"\n",
    "    returns the position of a substitutions relative to the start\n",
    "    of the protein sequence\n",
    "    \"\"\"\n",
    "    possible_sites = re.findall('\\(([^\\)]+)\\)', modified_seq)\n",
    "    best_site = np.argmax([float(i) for i in possible_sites])\n",
    "    modified_pos_prime = [m.start()-1 for m in re.finditer('\\(',modified_seq) ][best_site]\n",
    "    modified_pos = len(re.sub('\\(([^\\)]+)\\)', '', modified_seq[:modified_pos_prime]))\n",
    "    base_seq = re.sub('\\(([^\\)]+)\\)', '', modified_seq)\n",
    "    s = record_dict[protein].seq\n",
    "    seq_i = s.translate().find(base_seq)\n",
    "    i = seq_i + modified_pos\n",
    "    return i\n",
    "\n",
    "def find_positions_local(modified_seq, proteins):\n",
    "    \"\"\"\n",
    "    returns the position of a substitutions relative to the start\n",
    "    of the protein sequence, across all the codons\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    for prot in proteins.split(\" \"):\n",
    "        positions.append(str(find_substitution_position_local(modified_seq, prot)))\n",
    "    return \" \".join(positions)\n",
    "\n",
    "def is_gene(record):\n",
    "    \"\"\"\n",
    "    return True if the sequence satisfies the CDS requirement\n",
    "    \"\"\"\n",
    "    if len(record.seq)%3 != 0:\n",
    "        return False\n",
    "    if not record.seq[:3] in {'ATG','GTG','TTG','ATT','CTG'}:\n",
    "        return False\n",
    "    if record.seq[-3:].translate()!='*':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def refine_localization_probabilities(modified_seq, threshold = 0.05):\n",
    "    \"\"\"\n",
    "    returns the AAs that were possibly modified (with p > threshold).\n",
    "    Input: modified sequence (a string of AA with p of each to contain modification: APKV(.7)ML(.3)L means that V was modified with p = .7 and L with p = .3)\n",
    "    Output: A string with all candidate AAs separated by ';' (V;L).\n",
    "    \"\"\"\n",
    "    modified_sites = [modified_seq[m.start()-1] for m in re.finditer('\\(',modified_seq) ]\n",
    "    weights = [float(i) for i in re.findall('\\(([^\\)]+)\\)',modified_seq)]\n",
    "    site_probabilities = {}    \n",
    "    for aa, weight in zip(modified_sites, weights):\n",
    "        if aa in site_probabilities:\n",
    "            site_probabilities[aa] += weight\n",
    "        else:\n",
    "            site_probabilities[aa] = weight\n",
    "    return \";\".join([k for k,v in site_probabilities.items() if v>threshold])\n",
    "\n",
    "def c_term_probability(modified_sequence):\n",
    "    \"\"\"\n",
    "    Returns the probability that C term AA was modified.\n",
    "    \"\"\"\n",
    "    if modified_sequence[-1] == ')':\n",
    "        return float(modified_sequence[:-1].split('(')[-1])\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def n_term_probability(modified_sequence):\n",
    "    \"\"\"\n",
    "    Returns the probability that C term AA was modified.\n",
    "    \"\"\"\n",
    "    if modified_sequence[1] == '(':\n",
    "        return float(modified_sequence[2:].split(')')[0])\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def is_prot_nterm(sequence):\n",
    "    \"\"\"\n",
    "    Does the peptide originate at the protein's N-term\n",
    "    \"\"\"\n",
    "    for start in SA_search(sequence, W_aa, sa):\n",
    "        if W_aa[start-1] == '*':\n",
    "            return True\n",
    "        if W_aa[start-2] == '*':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_prot_cterm(sequence):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    l=len(sequence)\n",
    "    for start in SA_search(sequence, W_aa, sa):\n",
    "        end = start+l\n",
    "        if W_aa[end] == '*':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_codon_table():\n",
    "    return dict(zip(codons, amino_acids))\n",
    "\n",
    "def get_inverted_codon_table():\n",
    "    ct = get_codon_table()\n",
    "    inv_codon_table = {}\n",
    "    for k, v in ct.items():\n",
    "        inv_codon_table[v] = inv_codon_table.get(v, [])\n",
    "        inv_codon_table[v].append(k)\n",
    "    return inv_codon_table\n",
    "\n",
    "# 计算hamming距离\n",
    "def hamming(s1,s2): return sum(a!=b for a,b in zip(s1,s2))  \n",
    "\n",
    "def is_mispairing(row):\n",
    "    \"\"\"\n",
    "    Returns whether the substitution is mispairing or misloading, based on the\n",
    "    near-cognate mask.\n",
    "    \"\"\"\n",
    "    codon = row['codon']\n",
    "    destination = row['destination']\n",
    "    if pd.notnull(codon) and pd.notnull(destination):\n",
    "        if (codon in mask.index) and destination:\n",
    "            return mask.loc[codon,destination]\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return float('NaN')\n",
    "    \n",
    "def suffix_array(text, _step=32):\n",
    "    \"\"\"Analyze all common strings in the text.\n",
    "    \n",
    "    Short substrings of the length _step a are first pre-sorted. The are the \n",
    "    results repeatedly merged so that the garanteed number of compared\n",
    "    characters bytes is doubled in every iteration until all substrings are\n",
    "    sorted exactly.\n",
    "    \n",
    "    Arguments:\n",
    "        text:  The text to be analyzed.\n",
    "        _step: Is only for optimization and testing. It is the optimal length\n",
    "               of substrings used for initial pre-sorting. The bigger value is\n",
    "               faster if there is enough memory. Memory requirements are\n",
    "               approximately (estimate for 32 bit Python 3.3):\n",
    "                   len(text) * (29 + (_size + 20 if _size > 2 else 0)) + 1MB\n",
    "    \n",
    "    Return value:      (tuple)\n",
    "      (sa, rsa, lcp)\n",
    "        sa:  Suffix array                  for i in range(1, size):\n",
    "               assert text[sa[i-1]:] < text[sa[i]:]\n",
    "        rsa: Reverse suffix array          for i in range(size):\n",
    "               assert rsa[sa[i]] == i\n",
    "        lcp: Longest common prefix         for i in range(1, size):\n",
    "               assert text[sa[i-1]:sa[i-1]+lcp[i]] == text[sa[i]:sa[i]+lcp[i]]\n",
    "               if sa[i-1] + lcp[i] < len(text):\n",
    "                   assert text[sa[i-1] + lcp[i]] < text[sa[i] + lcp[i]]\n",
    "    >>> suffix_array(text='banana')\n",
    "    ([5, 3, 1, 0, 4, 2], [3, 2, 5, 1, 4, 0], [0, 1, 3, 0, 0, 2])\n",
    "    \n",
    "    Explanation: 'a' < 'ana' < 'anana' < 'banana' < 'na' < 'nana'\n",
    "    The Longest Common String is 'ana': lcp[2] == 3 == len('ana')\n",
    "    It is between  tx[sa[1]:] == 'ana' < 'anana' == tx[sa[2]:]\n",
    "    \"\"\"\n",
    "    tx = text\n",
    "    size = len(tx)\n",
    "    step = min(max(_step, 1), len(tx))\n",
    "    sa = list(range(len(tx)))\n",
    "    sa.sort(key=lambda i: tx[i:i + step])\n",
    "    grpstart = size * [False] + [True]  # a boolean map for iteration speedup.\n",
    "    # It helps to skip yet resolved values. The last value True is a sentinel.\n",
    "    rsa = size * [None]\n",
    "    stgrp, igrp = '', 0\n",
    "    for i, pos in enumerate(sa):\n",
    "        st = tx[pos:pos + step]\n",
    "        if st != stgrp:\n",
    "            grpstart[igrp] = (igrp < i - 1)\n",
    "            stgrp = st\n",
    "            igrp = i\n",
    "        rsa[pos] = igrp\n",
    "        sa[i] = pos\n",
    "    grpstart[igrp] = (igrp < size - 1 or size == 0)\n",
    "    while grpstart.index(True) < size:\n",
    "        # assert step <= size\n",
    "        nextgr = grpstart.index(True)\n",
    "        while nextgr < size:\n",
    "            igrp = nextgr\n",
    "            nextgr = grpstart.index(True, igrp + 1)\n",
    "            glist = []\n",
    "            for ig in range(igrp, nextgr):\n",
    "                pos = sa[ig]\n",
    "                if rsa[pos] != igrp:\n",
    "                    break\n",
    "                newgr = rsa[pos + step] if pos + step < size else -1\n",
    "                glist.append((newgr, pos))\n",
    "            glist.sort()\n",
    "            for ig, g in groupby(glist, key=itemgetter(0)):\n",
    "                g = [x[1] for x in g]\n",
    "                sa[igrp:igrp + len(g)] = g\n",
    "                grpstart[igrp] = (len(g) > 1)\n",
    "                for pos in g:\n",
    "                    rsa[pos] = igrp\n",
    "                igrp += len(g)\n",
    "        step *= 2\n",
    "    del grpstart\n",
    "    del rsa\n",
    "    return sa\n",
    "\n",
    "def SA_search(P, W, sa):\n",
    "    lp = len(P)\n",
    "    n = len(sa)\n",
    "    l = 0; r = n\n",
    "#     print(l,r)\n",
    "    while l < r:\n",
    "        mid = int((l+r) / 2)\n",
    "        a = sa[mid]\n",
    "        if P > W[a : a + lp]:\n",
    "            l = mid + 1\n",
    "        else:\n",
    "            r = mid\n",
    "    s = l; r = n\n",
    "    while l < r:\n",
    "        mid = int((l+r) / 2)\n",
    "        a = sa[mid]\n",
    "        if P < W[a : a + lp]:\n",
    "            r = mid\n",
    "        else:\n",
    "            l = mid + 1\n",
    "    return [sa[i] for i in range(s, r)]\n",
    "\n",
    "def find_homologous_peptide(P):\n",
    "    \"\"\"\n",
    "    Gets a peptide and returns whether it has homolegous in the genome.\n",
    "    If so, that peptide is discarded.\n",
    "    \"\"\"\n",
    "    if len(SA_search('K' + P, W_aa_ambiguous, sa_ambiguous)) > 0:\n",
    "        return False\n",
    "    if len(SA_search('R' + P, W_aa_ambiguous, sa_ambiguous)) > 0:\n",
    "        return False\n",
    "    if len(SA_search('*' + P, W_aa_ambiguous, sa_ambiguous)) > 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def create_modified_seq(modified_seq, destination):\n",
    "    \"\"\"\n",
    "    Input: base sequence with probabilities of substitution for each AA, observed destination AA.\n",
    "    Output: modified sequence.\n",
    "    (ACY(0.1)KFL(0.9)S, Y) -> ACYKFYS\n",
    "    \"\"\"\n",
    "    possible_sites = re.findall('\\(([^\\)]+)\\)', modified_seq)\n",
    "    best_site = np.argmax([float(i) for i in possible_sites])\n",
    "    modified_pos_prime = [m.start()-1 for m in re.finditer('\\(',modified_seq) ][best_site]\n",
    "    modified_pos = len(re.sub('\\(([^\\)]+)\\)', '', modified_seq[:modified_pos_prime]))\n",
    "    base_seq = re.sub('\\(([^\\)]+)\\)', '', modified_seq)\n",
    "    return base_seq[:modified_pos] + destination + base_seq[modified_pos+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bases = 'TCAG'\n",
    "codons = [a+b+c for a in bases for b in bases for c in bases]\n",
    "amino_acids = 'FFLLSSSSYY**CC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG' #corresponds to codons\n",
    "RC = {'A':'T', 'C':'G', 'G':'C', 'T':'A'}\n",
    "\n",
    "codon_table = get_codon_table()\n",
    "inverted_codon_table = get_inverted_codon_table()\n",
    "inverted_codon_table['L'] = inverted_codon_table['L'] + inverted_codon_table['I']\n",
    "MW_dict = {\"G\": 57.02147, \n",
    "            \"A\" : 71.03712, \n",
    "            \"S\" : 87.03203, \n",
    "            \"P\" : 97.05277, \n",
    "            \"V\" : 99.06842, \n",
    "            \"T\" : 101.04768, \n",
    "            \"I\" : 113.08407, \n",
    "            \"L\" : 113.08407, \n",
    "            \"N\" : 114.04293, \n",
    "            \"D\" : 115.02695, \n",
    "            \"Q\" : 128.05858, \n",
    "            \"K\" : 128.09497, \n",
    "            \"E\" : 129.0426, \n",
    "            \"M\" : 131.04049, \n",
    "            \"H\" : 137.05891,\n",
    "            \"F\" : 147.06842, \n",
    "            \"R\" : 156.10112, \n",
    "            \"C\" : 160.030654, #CamCys\n",
    "            \"Y\" : 163.0633,\n",
    "            \"W\" : 186.07932,\n",
    "            }# Molecular mass of amino acid residues\n",
    "\n",
    "subs_dict = { i+' to '+j : MW_dict[j] - MW_dict[i] for i in MW_dict for j in MW_dict if i!=j}\n",
    "del subs_dict['L to I']\n",
    "del subs_dict['I to L']\n",
    "# Leucine and isoleucine residues have the same molecular weight, and changes in both amino acids cannot be detected.\n",
    "\n",
    "for k,v in list(subs_dict.items()): # unifies I and L\n",
    "    if k[-1]=='I':\n",
    "        subs_dict[k+'/L'] = v\n",
    "        del subs_dict[k]\n",
    "        del subs_dict[k[:-1]+'L']\n",
    "\n",
    "sorted_subs, sorted_sub_masses = zip(*sorted(subs_dict.items(), key= lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Loads CDS fasta file and builds records of genes within it \"\"\"\n",
    "record_list = []\n",
    "translated_record_list = []\n",
    "names_list = []\n",
    "record_dict = {}\n",
    "boundaries_aa = [0]\n",
    "W_codons = []\n",
    "for record in SeqIO.parse(open(path_to_fasta,'rU'),'fasta'):\n",
    "    record.seq = record.seq.upper()    \n",
    "    if is_gene(record):\n",
    "        translation = str(record.seq.translate())\n",
    "        bits = record.description.split(' ')\n",
    "        for i in bits:\n",
    "            if 'gene=' in i:\n",
    "                record.name = i.split('=')[-1].strip(']')\n",
    "        record_list.append(record)\n",
    "        translated_record_list.append(translation) # amino acid sequence\n",
    "        names_list.append(record.name) # gene name\n",
    "        record_dict[record.name] = record\n",
    "        boundaries_aa.append(boundaries_aa[-1]+len(translation))\n",
    "        W_codons.extend(list(codonify(record.seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boundaries_aa = np.array(boundaries_aa[1:]) # an array annotating the genes' cumulative length\n",
    "W_aa = ''.join(translated_record_list)\n",
    "sa = suffix_array(W_aa)\n",
    "W_aa_ambiguous = W_aa.replace('I','L')\n",
    "sa_ambiguous = suffix_array(W_aa_ambiguous)\n",
    "\n",
    "AAs = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "sites = list(AAs)+['nterm','cterm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dp_columns = ['Raw file', 'Charge', 'm/z', 'Retention time', 'DP base sequence',\n",
    "              'DP mass difference', 'DP time difference', 'DP PEP', 'DP probabilities', \n",
    "              'DP positional probability', 'DP decoy'] # Column names may differ depending on the MaxQuant version used\n",
    "\n",
    "dp_df = pd.read_csv(path_to_allPeptides,sep=\"\\t\",usecols=dp_columns)\n",
    "dp = dp_df[pd.notnull(dp_df['DP mass difference'])]\n",
    "dp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dp['DPMD'] = dp['DP mass difference']\n",
    "dp['DPAA_noterm'] = dp['DP probabilities'].map(refine_localization_probabilities)\n",
    "dp['nterm'] = dp['DP probabilities'].map(n_term_probability) # p(N-term AA was substituted)\n",
    "dp['cterm'] = dp['DP probabilities'].map(c_term_probability)\n",
    "dp['prot_nterm'] = dp['DP base sequence'].map(is_prot_nterm) # Does the peptide come from the N-term of the protein\n",
    "dp['prot_cterm'] = dp['DP base sequence'].map(is_prot_cterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Handles mass differences that can be explained as PTMs \"\"\"\n",
    "danger_mods = pd.read_pickle(path_to_danger_mods)\n",
    "\n",
    "dp['danger'] = False\n",
    "for mod in danger_mods.iterrows():\n",
    "    mod = mod[1]\n",
    "    position = mod['position']\n",
    "    site = mod['site']\n",
    "    delta_m = mod['delta_m']\n",
    "    \n",
    "    mass_filter = (delta_m - (2*tol) < dp.DPMD) & (dp.DPMD < delta_m + (2*tol))\n",
    "    \n",
    "    term_filter = True\n",
    "    if position == 'Protein N-term':\n",
    "        term_filter = (dp.nterm > n_term_prob_cutoff) & dp.prot_nterm\n",
    "    elif position == 'Protein C-term':\n",
    "        term_filter = (dp.cterm > c_term_prob_cutoff) & dp.prot_cterm\n",
    "    elif position == 'Any N-term':\n",
    "        term_filter = dp.nterm > n_term_prob_cutoff\n",
    "    elif position == 'Any C-term':\n",
    "        term_filter = dp.cterm > c_term_prob_cutoff\n",
    "    \n",
    "    site_filter = True\n",
    "    if site in amino_acids:\n",
    "        site_filter = dp.DPAA_noterm.str.contains(site)\n",
    "    \n",
    "    dp.loc[site_filter & term_filter & mass_filter, 'danger'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" mass differences that could be explained as substitution \"\"\"\n",
    "dp['substitution'] = False\n",
    "for i in sorted(subs_dict.keys()):\n",
    "    delta_m = subs_dict[i]\n",
    "    original_aa = i[0]\n",
    "    dp.loc[(dp.DPMD > delta_m - tol) & (dp.DPMD < delta_m + tol) & (dp['DPAA_noterm'] == original_aa) & (dp['DP positional probability']>0.95) & ~dp['danger'], 'substitution'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_df = dp.copy()\n",
    "# remove PTM\n",
    "print('Original info')\n",
    "tmp_df = tmp_df[tmp_df['danger']!=True]\n",
    "print('remove PTM')\n",
    "\n",
    "print('Could be explained by subtitutions')\n",
    "\n",
    "for i in sorted(subs_dict.keys()):\n",
    "    delta_m = subs_dict[i]\n",
    "    original_aa = i[0]\n",
    "    tmp_df.loc[(tmp_df.DPMD > delta_m - tol) & (tmp_df.DPMD < delta_m + tol) & (tmp_df['DPAA_noterm'] == original_aa), 'substitution'] = True\n",
    "    \n",
    "tmp_df = tmp_df[tmp_df['substitution']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create mask for mispairing. A binary dataframe indicating for each codon the AAs encoded by near-cognate codons. \n",
    "\"\"\"            \n",
    "mask = pd.DataFrame(data = False, index = codons, columns=list('ACDEFGHKLMNPQRSTVWY'),dtype=float)    \n",
    "for label in codons:\n",
    "    near_cognates = np.array([hamming(i,label)==1 for i in codons]) # 相近密码子\n",
    "    reachable_aa = set(np.array(list(amino_acids))[near_cognates])\n",
    "    mask.loc[label] =[i in reachable_aa for i in 'ACDEFGHKLMNPQRSTVWY']\n",
    "\n",
    "for label in mask.index: # removes \"near-cognates\" that encodes the same AA\n",
    "    for col in mask.columns:\n",
    "        if label in inverted_codon_table[col]:\n",
    "            mask.loc[label, col] = float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs = dp[dp['substitution']!=False].copy() # after filtering\n",
    "subs['proteins'] = subs['DP base sequence'].map(find_proteins) \n",
    "subs['protein'] = subs['proteins'].map(lambda x: x.split(' ')[0] if len(x)>0 else float('NaN'))\n",
    "subs = subs[pd.notnull(subs['protein'])] #mismatching files\n",
    "\n",
    "subs['codons'] = float('NaN')\n",
    "subs.loc[ subs['DP positional probability'] > positional_probability_cutoff, 'codons' ] = subs[ subs['DP positional probability'] > positional_probability_cutoff ]['DP probabilities'].map(fetch_best_codons)\n",
    "subs['codon'] = subs['codons'].map(lambda x: x.split(' ')[0] if len(set(x.split(' ')))==1 else float('NaN')) \n",
    "subs['destination'] = subs['substitution'].map(lambda x: x[-1] if x else False)\n",
    "subs['origin'] = subs['substitution'].map(lambda x: x[0] if x else False)\n",
    "subs['mispairing'] = subs.apply(is_mispairing,1)    \n",
    "\n",
    "subs['positions'] = subs.apply(lambda row : \n",
    "        find_positions_local(row['DP probabilities'],row['proteins']),\n",
    "        axis=1)\n",
    "subs['position'] = subs['positions'].map(lambda x: int(x.split(' ')[0]) if len(set(x.split(' ')))==1 else float('NaN')) \n",
    "subs['modified_sequence'] = subs.apply(lambda row : create_modified_seq(row['DP probabilities'], row['destination']), axis=1)\n",
    "subs['modified_sequence'] = subs['modified_sequence'].map(lambda x: x.replace('I','L'))\n",
    "subs = subs[subs['modified_sequence'].map(lambda x: find_homologous_peptide(x))] \n",
    "\n",
    "subs.sort_values('DP PEP', inplace = True)\n",
    "subs['decoy'] = pd.notnull(subs['DP decoy'])\n",
    "cut_off = np.max(np.where(np.array([i/float(j) for i,j in zip(subs['decoy'].cumsum(), range(1,len(subs)+1))])<fdr))\n",
    "subs = subs.iloc[:cut_off+1]\n",
    "\n",
    "subs_raw = subs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs_reps = subs.copy()\n",
    "subs_reps = subs_reps[subs_reps['DP decoy']!='+']\n",
    "subs_reps = subs_reps[pd.notnull(subs_reps['codon'])]\n",
    "\n",
    "# 将样本和文件进行对应\n",
    "columns_summary = ['Raw file', 'Experiment']\n",
    "summary = pd.read_csv(path_to_summary,sep=\"\\t\",usecols = columns_summary)\n",
    "summary.columns = ['Raw file','Sample']\n",
    "subs_reps = pd.merge(subs_reps,summary,on='Raw file')\n",
    "\n",
    "usedCategories = [\n",
    "    '0_2h','4_6h','10_12h','18_20h','44_46h','66_68h','83_85h','l3',\n",
    "    'p1', 'p2','p3', 'p4', 'p5','vf_male','vf_female','male','female'\n",
    "]\n",
    "len(usedCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs_reps['condition'] = subs_reps['Sample'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "subs_reps = subs_reps[subs_reps['condition'].isin(usedCategories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "editing_related_sites_df = pd.read_csv(\"editing_related_sites_AllSamples.csv\",sep=\"\\t\")\n",
    "subs_reps = subs_reps[~(subs_reps['proteins'].isin(editing_related_sites_df['proteins']))&\n",
    "               ~(subs_reps['position'].isin(editing_related_sites_df['position']))]\n",
    "\n",
    "subs_reps_tmp = subs_reps[['Sample','substitution','protein','codon','position']].drop_duplicates().copy()\n",
    "\n",
    "subs_reps_tmp_count = subs_reps_tmp.groupby(['substitution','protein','codon','position'])['Sample'].count().reset_index()\n",
    "\n",
    "subs_reps_tmp_count = subs_reps_tmp_count[subs_reps_tmp_count['substitution'] != 'A to I/L'] # this substitution is beyond detection\n",
    "\n",
    "sample_count = subs_reps_tmp_count['Sample'].value_counts().reset_index()\n",
    "sample_count.columns = ['DetectedSamples','Count']\n",
    "\n",
    "tmp_df = pd.DataFrame(columns=['DetectedSamples'])\n",
    "tmp_df['DetectedSamples'] = [i for i in range(1,69)]\n",
    "sample_count = pd.merge(sample_count,tmp_df,how='outer')\n",
    "sample_count.fillna(0,inplace=True)\n",
    "sample_count['log2Count'] = np.log2(sample_count['Count']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_count_new = sample_count[sample_count['DetectedSamples']<12][['DetectedSamples','Count']]\n",
    "new_row = {'DetectedSamples':'12~68','Count':sample_count[sample_count['DetectedSamples']>11]['Count'].sum()}\n",
    "sample_count_new = sample_count_new.append(new_row,ignore_index=True)\n",
    "sample_count_new['DetectedSamples'] = sample_count_new['DetectedSamples'].astype(str)\n",
    "sample_count_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "sns.barplot(x='DetectedSamples',y='Count',data=sample_count_new,color='steelblue')\n",
    "plt.xlabel(\"the number of detected samples\")\n",
    "plt.ylabel(\"the count of substitutions\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs_reps_tmp_clean = subs_reps_tmp_count[subs_reps_tmp_count['Sample']<4] \n",
    "\n",
    "del subs_reps_tmp_clean['Sample']\n",
    "\n",
    "# print(subs_reps.shape)\n",
    "\n",
    "subs_reps_clean = pd.merge(subs_reps,subs_reps_tmp_clean,on=['substitution','protein','codon','position'])\n",
    "\n",
    "# print(subs_reps_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Error Count\n",
    "subs_reps = subs_reps_clean.copy()\n",
    "subs_reps['Sample'] = subs_reps['Raw file'].apply(lambda x:re.search(r\"set[12]_(.*)\",x).group(1))\n",
    "\n",
    "count_info = subs_reps['Sample'].value_counts().reset_index()\n",
    "count_info.columns = ['Sample','Count']\n",
    "count_info['label'] = count_info['Sample'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "\n",
    "\n",
    "LabelList = ['0_2h','4_6h','10_12h','18_20h',\n",
    "             'L1_44_46h','L2_66_68h','eL3_83_85h','lateL3',\n",
    "             'P1','P2','P3','P4','P5',\n",
    "             'vF_male','vF_female','F_male','F_female']\n",
    "\n",
    "count_info['label'] = count_info['label'].astype(\"category\").cat.set_categories(LabelList)\n",
    "count_info.sort_values(\"label\",inplace=True)\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "sns.barplot(x='label',y='Count',data=count_info,color='steelblue',alpha=0.8,ci=68)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Substitution sites detected\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_count_matrix(df):\n",
    "\n",
    "    matrix = pd.DataFrame(data = 0, index = codons, columns=list('ACDEFGHKLMNPQRSTVWY'),dtype=float)\n",
    "    df = df[df['DP decoy']!='+']\n",
    "    df = df[pd.notnull(df['codon'])] # filter \n",
    "    df['codon'] = df['codon'].map(lambda x: x.replace('T','U'))\n",
    "    for label in matrix.index:\n",
    "        if codon_table[label] == '*':\n",
    "            matrix.loc[label] = float('NaN')\n",
    "        for col in matrix.columns:\n",
    "            if (label in inverted_codon_table[col]) or (codon_table[label] +' to '+col in exact_PTM_spec_list):\n",
    "                matrix.loc[label, col] = float('NaN')\n",
    "    subs_agg = pd.DataFrame(df.groupby(['protein','position','origin','destination','codon']).groups.keys(), columns=['protein','position','origin','destination','codon'])\n",
    "\n",
    "    for x, l in subs_agg.groupby(['codon', 'destination']).groups.items():\n",
    "        codon, destination = x\n",
    "        if (codon in matrix.index) and pd.notnull(matrix.loc[codon,destination]):\n",
    "            matrix.loc[codon,destination] = len(l)\n",
    "    matrix.rename(columns={\"L\": \"I/L\"},inplace=True)\n",
    "    return matrix\n",
    "    \n",
    "def probe_mismatch(codon1, codon2, pos, spec):\n",
    "    origin, destination = spec\n",
    "    for i in range(3):\n",
    "        if i == pos:\n",
    "            if codon1[i] != origin or codon2[i] != destination:\n",
    "                return False\n",
    "        else:\n",
    "            if codon1[i] != codon2[i]:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "bases = 'UCAG'\n",
    "codons = [a+b+c for a in bases for b in bases for c in bases]\n",
    "\n",
    "amino_acids = 'FFLLSSSSYY**CC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG'\n",
    "RC = {'A':'U', 'C':'G', 'G':'C', 'U':'A'}\n",
    "\n",
    "codon_table = get_codon_table()\n",
    "inverted_codon_table = get_inverted_codon_table()\n",
    "inverted_codon_table['L'] = inverted_codon_table['L'] + inverted_codon_table['I']\n",
    "tol = 0.005\n",
    "\n",
    "aas_sorted_by_mass = [i[0] for i in sorted(MW_dict.items(),key=lambda x:x[1])]\n",
    "danger_mods = pd.read_pickle('danger_mods')\n",
    "exact_PTM_spec = pd.DataFrame(index = aas_sorted_by_mass,\n",
    "                              columns = aas_sorted_by_mass,\n",
    "                              dtype = int)\n",
    "\n",
    "for aa1 in MW_dict.keys():\n",
    "    for aa2 in MW_dict.keys():\n",
    "        delta_m = MW_dict[aa2] - MW_dict[aa1]\n",
    "        exact_PTM_spec.loc[aa1,aa2]=len(danger_mods[(danger_mods['delta_m']<delta_m + 0.0005) & (danger_mods['delta_m']>delta_m - 0.0005) & (danger_mods['site']==aa1)]) > 0\n",
    "\n",
    "exact_PTM_spec_list = [str(i) + ' to ' + str(j) for i in aas_sorted_by_mass for j in  aas_sorted_by_mass if exact_PTM_spec.loc[i,j]] \n",
    "\n",
    "mask = pd.DataFrame(data = False,\n",
    "                    index = codons,\n",
    "                    columns = list('ACDEFGHKLMNPQRSTVWY'),\n",
    "                    dtype = float)\n",
    "\n",
    "for label in codons:\n",
    "    near_cognates = np.array([hamming(i,label)==1 for i in codons])\n",
    "    reachable_aa = set(np.array(list(amino_acids))[near_cognates])\n",
    "    mask.loc[label] =[i in reachable_aa for i in 'ACDEFGHKLMNPQRSTVWY']\n",
    "    \n",
    "for label in mask.index:\n",
    "    if codon_table[label] == '*':\n",
    "        mask.loc[label]=float('NaN')\n",
    "    for col in mask.columns:\n",
    "        if (label in inverted_codon_table[col]) or (codon_table[label] +' to '+col in exact_PTM_spec_list):\n",
    "            mask.loc[label, col] = float('NaN')\n",
    "\n",
    "subs_reps = subs_reps[~subs_reps.decoy]\n",
    "\n",
    "data = prepare_count_matrix(subs_reps)\n",
    "data.to_csv(\"../Results/data.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_data = np.log2(data+1)\n",
    "m = log_data.max().max()\n",
    "\n",
    "log_data_tmp = log_data.copy()\n",
    "log_data_tmp.reset_index(inplace=True)\n",
    "log_data_tmp['protein'] = log_data_tmp['index'].apply(lambda x: str(Seq.Seq(x).translate()))\n",
    "log_data_tmp['label'] = log_data_tmp.apply(lambda x: x['protein']+'-'+x['index'],axis=1)\n",
    "log_data_tmp.index = log_data_tmp['label']\n",
    "log_data_tmp.drop(columns=['index','protein','label'],inplace=True)\n",
    "log_data_tmp.head(2)\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "font = {'size': 13}\n",
    "# sns.set_style({'font.family':'sans-serif', 'font.serif':['Arial']})\n",
    "fig, ax = plt.subplots(figsize= (13,13))\n",
    "ax = sns.heatmap(log_data_tmp,square = True, cmap = 'Reds', ax = ax, cbar_kws = {\"shrink\": .2},linewidths=2)\n",
    "ax.set_ylabel('original codon', fontdict=font)\n",
    "ax.set_xlabel('destination amino acid', fontdict=font)\n",
    "ax.tick_params(labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Rate Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mf = pd.read_csv(path_to_matched_features,\n",
    "                 sep = '\\t')\n",
    "mf.replace(0, np.nan, inplace = True)\n",
    "\n",
    "pep_head = pd.read_csv(path_to_peptides,\n",
    "                  sep = '\\t', \n",
    "                  nrows = 10,\n",
    "                  index_col = 'Sequence')\n",
    "\n",
    "intensities = [i for i in pep_head.columns if 'Intensity ' in i]\n",
    "samples = [i.split()[-1] for i in intensities]\n",
    "\n",
    "pep_columns = ['Sequence', 'Intensity', 'Evidence IDs'] + intensities\n",
    "\n",
    "pep = pd.read_csv(path_to_peptides,\n",
    "                  sep = '\\t', \n",
    "                  usecols = pep_columns,\n",
    "                  index_col = 'Sequence')\n",
    "\n",
    "pep.replace(0, np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs_reps = subs_raw.copy()\n",
    "subs_reps = subs_reps[subs_reps['DP decoy']!='+']\n",
    "subs_reps = subs_reps[pd.notnull(subs_reps['codon'])]\n",
    "\n",
    "columns_summary = ['Raw file', 'Experiment']\n",
    "summary = pd.read_csv(path_to_summary,sep=\"\\t\",usecols = columns_summary)\n",
    "summary.columns = ['Raw file','Sample']\n",
    "subs_reps = pd.merge(subs_reps,summary,on='Raw file')\n",
    "\n",
    "subs_reps['condition'] = subs_reps['Sample'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "# remove editing related sites\n",
    "editing_related_sites_df = pd.read_csv(\"../Data/editing_related_sites_AllSamples.csv\",sep=\"\\t\")\n",
    "subs_reps = subs_reps[~(subs_reps['proteins'].isin(editing_related_sites_df['proteins']))&\n",
    "               ~(subs_reps['position'].isin(editing_related_sites_df['position']))]\n",
    "\n",
    "subs_reps_tmp = subs_reps[['Sample','substitution','protein','codon','position']].drop_duplicates().copy()\n",
    "\n",
    "subs_reps_tmp_count = subs_reps_tmp.groupby(['substitution','protein','codon','position'])['Sample'].count().reset_index()\n",
    "\n",
    "subs_reps_tmp_count = subs_reps_tmp_count[subs_reps_tmp_count['substitution'] != 'A to I/L'] # this substitution is beyond detection\n",
    "\n",
    "subs_reps_tmp_clean = subs_reps_tmp_count[subs_reps_tmp_count['Sample']<6]\n",
    "\n",
    "\n",
    "del subs_reps_tmp_clean['Sample']\n",
    "subs_reps_clean = pd.merge(subs_reps,subs_reps_tmp_clean,on=['substitution','protein','codon','position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_cds_seq(base_seq,protein):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "#     print(base_seq,protein)\n",
    "    s = record_dict[protein].seq\n",
    "    tmp_pep = s.translate()\n",
    "    seq_i = s.translate().find(base_seq)\n",
    "    tmp_cds = str(s[seq_i*3:(len(base_seq)+seq_i)*3])\n",
    "    return(tmp_cds)\n",
    "\n",
    "def find_CDSs_local(base_seq, proteins):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    CDSs = []\n",
    "    if proteins == '': # if peptide could not mapped into any known protein\n",
    "        return False\n",
    "    else:\n",
    "        \n",
    "        for prot in proteins.split(\" \"):\n",
    "            CDSs.append(str(find_cds_seq(base_seq, prot)))\n",
    "        return \" \".join(CDSs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pep_df = pep.copy()\n",
    "pep_df.reset_index(inplace=True)\n",
    "pep_df['CDSs'] = pep_df['Sequence'].apply(lambda x: find_CDSs_local(x,find_proteins(x))) # may take 15 mins?\n",
    "\n",
    "pep_df = pep_df[pep_df['CDSs'] != False]\n",
    "pep_df['CDS'] = pep_df['CDSs'].apply(lambda x: x.split(' ')[0] if len(set(x.split(' '))) == 1 else False)\n",
    "pep_df = pep_df[pep_df['CDS']!=False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Codons = [i.replace('U','T') for i in codons]\n",
    "# codons.replace('U','T')\n",
    "\n",
    "BP_Intensity = {}\n",
    "\n",
    "for sample in intensities:\n",
    "    BP_Intensity[sample] = {}\n",
    "    for Codon in Codons:\n",
    "        BP_Intensity[sample][Codon] = 0\n",
    "        \n",
    "# assign intensity into each codon of each experiment\n",
    "for index,row in pep_df.iterrows():\n",
    "    for codon in codonify(row['CDS']):\n",
    "        for sample in intensities:\n",
    "#             print(row[sample])\n",
    "            if not np.isnan(row[sample]):\n",
    "#                 print(row[sample])\n",
    "                BP_Intensity[sample][codon] += row[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_summary = ['Raw file', 'Experiment']\n",
    "summary = pd.read_csv(path_to_summary,sep=\"\\t\",usecols = columns_summary)\n",
    "summary.columns = ['Raw file','Sample']\n",
    "\n",
    "\"\"\"\n",
    "matchedFeatured.txt contains features that weren't identified and appeared in more than one sample.\n",
    "Those features have both uncalibrated and calibrated retention time.\n",
    "subs, which is derived from allPeptides.txt, has only uncalibrated retention time;\n",
    "to extract features from mf.txt, retention times are calibrated using evidence.txt\n",
    "\"\"\"\n",
    "columns_evidence = ['Raw file', 'Retention time', 'Calibrated retention time', 'Intensity', 'Charge']\n",
    "\n",
    "evidence = pd.read_csv(path_to_evidence,\n",
    "                       sep = '\\t', usecols = columns_evidence)\n",
    "\n",
    "# evidence['Sample'] = evidence['Raw file'].map(lambda x: x.split('_')[-2]) # \n",
    "# Sample \n",
    "evidence = pd.merge(evidence,summary,on='Raw file')\n",
    "\n",
    "calibrate = {}\n",
    "for i,j in evidence.groupby('Raw file'): # RT for each file (fraction) is being calibrated\n",
    "    calibrate[i] = interp1d(j['Retention time'], j['Calibrated retention time'],fill_value=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs_tmp = subs_reps_clean.copy()\n",
    "\n",
    "subs_tmp['modified_sequence'] = subs_tmp.apply(lambda row : create_modified_seq(row['DP probabilities'], row['destination']), axis=1) \n",
    "subs_tmp['base RT'] =  subs_tmp['Retention time'] - subs_tmp['DP time difference']\n",
    "\n",
    "for i,j in subs_tmp.groupby('Raw file'):\n",
    "    subs_tmp.loc[j.index, 'Calibrated retention time'] = subs_tmp.loc[j.index, 'Retention time'].map(lambda x: calibrate[i](x))\n",
    "    subs_tmp.loc[j.index, 'Calibrated base RT'] = subs_tmp.loc[j.index, 'base RT'].map(lambda x: calibrate[i](x))\n",
    "\n",
    "subs_tmp['Calibrated DPTD'] = subs_tmp['Calibrated retention time'] - subs_tmp['Calibrated base RT']\n",
    "subs_tmp['Base m/z'] = subs_tmp['m/z'] - (subs_tmp['DPMD']/subs_tmp['Charge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs_codon = subs_tmp[~subs_tmp['codon'].isnull()] \n",
    "\n",
    "DP_Intensity = {}\n",
    "DP_DetectCount = {} # \n",
    "\n",
    "for sample in intensities:\n",
    "    DP_Intensity[sample] = {}\n",
    "    DP_DetectCount[sample] = {}\n",
    "    for Codon in Codons:\n",
    "        DP_Intensity[sample][Codon] = 0\n",
    "        DP_DetectCount[sample][Codon] = 0\n",
    "\n",
    "gb_codon = subs_codon.groupby(['Charge','DP base sequence','modified_sequence'])\n",
    "\n",
    "for i,j in gb_codon:\n",
    "    charge, base_sequence, modified_sequence = i\n",
    "    ref_dp = j.iloc[j['DP PEP'].argmin()] # \n",
    "    charge, dp_mz, base_sequence, dp_rt, codon = ref_dp[['Charge','m/z','DP base sequence','Calibrated retention time','codon']]\n",
    "    \n",
    "    f1 = mf['Calibrated retention time'] < dp_rt + rt_tol\n",
    "    f2 = mf['Calibrated retention time'] > dp_rt - rt_tol\n",
    "    f3 = mf['m/z'] < dp_mz * (1 + mz_tol)\n",
    "    f4 = mf['m/z'] > dp_mz * (1 - mz_tol)\n",
    "    f5 = mf['Charge'] == charge\n",
    "    \n",
    "    m = mf[f1 & f2 & f3 & f4 & f5] #\n",
    "    l = len(m)\n",
    "    \n",
    "    if l>=1:\n",
    "        for sample in intensities:\n",
    "            if not np.isnan(m[sample].values[0]):\n",
    "                DP_Intensity[sample][codon] += m[sample].values[0]\n",
    "                DP_DetectCount[sample][codon] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ErrorRate_codon = {}\n",
    "\n",
    "for sample in intensities:\n",
    "    ErrorRate_codon[sample] = {}\n",
    "    for Codon in Codons:\n",
    "        ErrorRate_codon[sample][Codon] = DP_Intensity[sample][Codon]/(BP_Intensity[sample][Codon]+1+DP_Intensity[sample][Codon])\n",
    "        \n",
    "ErrorRate_codon_df = pd.DataFrame(ErrorRate_codon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 17 Samples\n",
    "columns_df = pd.DataFrame(ErrorRate_codon_df.columns)\n",
    "columns_df.columns = ['label']\n",
    "columns_df['condition'] = columns_df['label'].apply(lambda x: '_'.join(x.split(' ')[1].split('_')[:-1]))\n",
    "usedCategories = [\n",
    "    '0_2h','4_6h','10_12h','18_20h','44_46h','66_68h','83_85h','l3',\n",
    "    'p1', 'p2','p3', 'p4', 'p5','vf_male','vf_female','male','female'\n",
    "]\n",
    "columns_df = columns_df[columns_df['condition'].isin(usedCategories)]\n",
    "\n",
    "ErrorRate_codon_used = ErrorRate_codon_df.loc[:,list(columns_df['label'].values)]\n",
    "ErrorRate_codon_used.to_csv(\"../Results/ErrorRate_tmp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
